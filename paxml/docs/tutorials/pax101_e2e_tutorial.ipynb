{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1"
   },
   "source": [
    "# Pax Workshop\n",
    "## Lab 6: Pax End-to-end Tutorial\n",
    "\n",
    "\n",
    "\n",
    "Goal: In this lab, you will put together all that you have learned so far. You will be building a Translator using a Transformer Encoder/Decoder Architecture using the Pax framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2"
   },
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is a tutorial on a few key ML concepts, including:\n",
    "\n",
    "* Transformers\n",
    "* JAX (which is so cool)\n",
    "* Pax\n",
    "\n",
    "Below we will be building a Translator using a Transformer Encoder/Decoder Architecture. The model is built using the Pax framework which I find pretty compelling for building and maintaining complex models and their associated configurations.\n",
    "\n",
    "JAX really makes development and debugging a lot easier. Being able to inspect all your arrays easily with simple `print()` statements make a tremendous difference. Using methods like `jax.pmap()`–which you will learn about below–makes running on multiple TPU's so easy.\n",
    "\n",
    "Before we get started, a note on how to use this notebook. I found that I learned better by starting with a blank notebook on a new tab and then coping 1 cell at a time, running, inspecting the variables, running JAX NumPy on it to understand the logic, etc. With this methodology, I got to understand both JAX and NumPy better, how to create a padding mask (even though Pax has helper methods), and many lower level details of working with these Frameworks.\n",
    "\n",
    "This notebook, assumes you have already done the starter notebook on JAX and Pax layers. We do not go too deep into the layer design here, but instead show how to build test and combine them to train a model.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Import important base libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/deepmind/einshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "# This notebook demonstrates key concepts in Pax\n",
    "\n",
    "import dataclasses\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from einshape import jax_einshape as einshape\n",
    "from praxis import base_hyperparams\n",
    "from praxis import base_layer\n",
    "from praxis import py_utils\n",
    "from praxis import layers\n",
    "from praxis import pax_fiddle\n",
    "from praxis import pytypes\n",
    "\n",
    "NestedMap = py_utils.NestedMap\n",
    "WeightInit = base_layer.WeightInit\n",
    "WeightHParams = base_layer.WeightHParams\n",
    "InstantiableParams = py_utils.InstantiableParams\n",
    "JTensor = pytypes.JTensor\n",
    "NpTensor = pytypes.NpTensor\n",
    "WeightedScalars = pytypes.WeightedScalars\n",
    "instantiate = base_hyperparams.instantiate\n",
    "\n",
    "# Standard prng key names\n",
    "PARAMS = base_layer.PARAMS\n",
    "RANDOM = base_layer.RANDOM\n",
    "\n",
    "key = jax.random.PRNGKey(seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4"
   },
   "source": [
    "# Multi-Headed Attention\n",
    "\n",
    "*Multi-headed attention* is one of the key concepts behind the Transformer Architecture. The paper from Google [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) is a really good starting point to learn about it.\n",
    "\n",
    "Before we get into multi-headed attention, let's first introduce the\n",
    "single-headed scaled dot-product attention layer. Multi-headed attention is largely an extension to single-headed attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5"
   },
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "![scaled_attention.png](data:image/png;base64,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)\n",
    "\n",
    "The *attention* function used by the transformer takes three inputs: $Q$ (query), $K$ (key), $V$ (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "\\begin{align}\n",
    "Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V\n",
    "\\end{align}\n",
    "\n",
    "(Ignore the slight abuse of notation as far as the running index $k$ goes: $d_k$ is meant to represent the query and key dimensionality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6"
   },
   "source": [
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because, for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\n",
    "\n",
    "For example, consider that $Q$ and $K$ have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of dk. So the square root of dk is used for scaling, so you get a consistent variance regardless of the value of dk. If the variance is too low the output may be too flat to optimize effectively. If the variance is too high the softmax may saturate at initialization making it difficult to learn.\n",
    "\n",
    "The input mask is assumed to contain a large negative value where the inputs need to be masked. This is done because the mask is summed with the scaled matrix multiplication of $Q$ and $K$ and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output. We use Pax helpers to accomplish this, but I have a quick detour on masks, right after this class to describe the different type of masks we will need to build our Translation model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(base_layer.BaseLayer):\n",
    "\n",
    "  \"\"\"Transformer multiheaded attention layer.\n",
    "\n",
    "  Attributes:\n",
    "    num_head: Number of heads for MultiHeadedAttention layer.\n",
    "    input_dim: Input dimension of attention layer.\n",
    "    atten_dropout_prob: Attention dropout probability.\n",
    "  \"\"\"\n",
    "  num_head: int = 0\n",
    "  input_dim: int = 0\n",
    "  atten_dropout_prob: float = 0.0\n",
    "\n",
    "  def setup(self) -> None:\n",
    "    if self.input_dim % self.num_head != 0:\n",
    "      raise ValueError(f'{self.input_dim=} must be a multiple of '\n",
    "                       f'{self.num_head=}')\n",
    "    self.depth = self.input_dim // self.num_head\n",
    "\n",
    "    # Create the linear layers.\n",
    "    # Each of the below `w` matrices is `concat(w_i for i in range(num_head))`.\n",
    "    self.create_child(\n",
    "        'wq',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Linear, input_dims=self.input_dim,\n",
    "            output_dims=self.input_dim))\n",
    "    self.create_child(\n",
    "        'wk',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Linear, input_dims=self.input_dim,\n",
    "            output_dims=self.input_dim))\n",
    "    self.create_child(\n",
    "        'wv',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Linear, input_dims=self.input_dim,\n",
    "            output_dims=self.input_dim))\n",
    "    self.create_child(\n",
    "        'dense',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Linear, input_dims=self.input_dim,\n",
    "            output_dims=self.input_dim))\n",
    "\n",
    "  @staticmethod\n",
    "  def dot_attention(\n",
    "      query: JTensor,\n",
    "      key: JTensor,\n",
    "      value: JTensor,\n",
    "      atten_mask: Optional[JTensor] = None) -> tuple:\n",
    "    \"\"\"Calculate the attention weights.\n",
    "\n",
    "    q, k, v must have matching leading dimensions (in this case batch size and\n",
    "    num_heads). k, v must have matching penultimate dimension, i.e.: seq_len_k =\n",
    "    seq_len_v.\n",
    "\n",
    "    The mask has different shapes depending on its type (padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args: Using the notation b = batch_size, n = num_heads, s = sequence length\n",
    "      of key and value, t = sequence length of query, h = dimension of heads,\n",
    "      the arguments of the shape must be as follows:\n",
    "          q: query shape == [b, n, t, h]\n",
    "          k: key shape   == [b, n, s, h]\n",
    "          v: value shape == [b, n, s, h]\n",
    "          mask: Float tensor with shape broadcastable to [..., t, s]. Defaults\n",
    "          to None.\n",
    "\n",
    "    Returns:\n",
    "      output with shape [b, n, t, h]\n",
    "      attention_weights with shape [b, n, t, s]\n",
    "    \"\"\"\n",
    "    # confirm we have the right input shapes\n",
    "    b, n, s, h = key.shape\n",
    "    t = query.shape[2]\n",
    "    base_layer.assert_has_shape(value, [b, n, s, h])\n",
    "    base_layer.assert_has_shape(query, [b, n, t, h])\n",
    "\n",
    "    logits = jnp.einsum('BNTH,BNSH->BNTS', query, key)\n",
    "    base_layer.assert_has_shape(logits, [b, n, t, s])\n",
    "\n",
    "    scaled_attention_logits = logits / jnp.sqrt(h)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if atten_mask is not None:\n",
    "      # If only padding bias is supplied, then atten_mask can be [B, 1, 1, S]\n",
    "      # since each target token is prohibited from attending to the same set of\n",
    "      # source tokens. In this case tiling is inefficient and unnecessary.\n",
    "      # If there is no padding mask, and only causal mask then the shape can be\n",
    "      # [1, 1, T, S]\n",
    "      chex.assert_shape(atten_mask, [{1, b}, 1, {1, t}, s])\n",
    "      scaled_attention_logits += atten_mask\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = jax.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = jnp.einsum('BNTS,BNSH->BNTH', attention_weights, value)\n",
    "    base_layer.assert_has_shape(output, [b, n, t, h])\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "  def split_heads(self, x: JTensor) -> JTensor:\n",
    "    \"\"\"Split and transpose a (b, l, hd) tensor into one with shape (b, h, l, d).\n",
    "\n",
    "\n",
    "    Split the last dimension into (num_heads, depth), then transpose the inner\n",
    "    two dimensions, returning a tensor with shape\n",
    "        ((b)atch_size, num_(h)eads, seq_(l)en, (d)epth).\n",
    "    \"\"\"\n",
    "    return einshape('bl(hd)->bhld', x, h=self.num_head)\n",
    "\n",
    "  def __call__(self, query: JTensor, key: JTensor, value: JTensor,\n",
    "               atten_mask: JTensor) -> tuple:\n",
    "    \"\"\"Compute multihead attention on the input query, key, and value.\n",
    "\n",
    "    Args: Using the notation b = batch_size, s = sequence length of key and\n",
    "      value, t = sequence length of query, d = dimension of key, query, value\n",
    "      features, the arguments of the shape must be as follows:\n",
    "          q: query shape == (b, t, d)\n",
    "          k: key shape   == (b, s, d)\n",
    "          v: value shape == (b, s, d)\n",
    "          atten_mask: Float tensor with shape broadcastable to (..., t, s).\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output attention with shape (b, t, d) - matching the query shape\n",
    "      attention weight with shape (b, t, s)\n",
    "    \"\"\"    \n",
    "    # Get dim sizes\n",
    "    (b, s, d), (_, t, _) = key.shape, query.shape\n",
    "\n",
    "    # Ensure that the input_tensor has an appropriate shape.\n",
    "    base_layer.assert_has_shape(key, [b, s, self.input_dim])\n",
    "    base_layer.assert_has_shape(value, [b, s, d])\n",
    "    base_layer.assert_has_shape(query, [b, t, d])\n",
    "\n",
    "    query = self.split_heads(self.wq(query))\n",
    "    key = self.split_heads(self.wk(key))\n",
    "    value = self.split_heads(self.wv(value))\n",
    "\n",
    "    base_layer.assert_has_shape(query, [b, self.num_head, t, self.depth])\n",
    "\n",
    "    scaled_attention, attention_weights = self.dot_attention(\n",
    "        query, key, value, atten_mask)\n",
    "    # Flip axes and concatenate the N separate heads back into a single vector\n",
    "    # before applying the dense feed-forward model.\n",
    "    concat_attention = einshape('bnth->bt(nh)', scaled_attention)\n",
    "    base_layer.assert_has_shape(concat_attention, [b, t, d])\n",
    "\n",
    "    output = self.dense(concat_attention)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8"
   },
   "source": [
    "The code above, which implements a full Pax layers, does quite a few things at once. Don't worry we will cover each shortly, but for now let's focus on the static class method called 'dot_attention'. And test it out on a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "def print_out(q: JTensor, k: JTensor, v: JTensor) -> None:\n",
    "  out, attn = MultiHeadedAttention.dot_attention(q, k, v, None)\n",
    "  print(f'Attention weights are: {attn}')\n",
    "  print(f'Output is: {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "# Note that the rows here are orthogonal to each other.\n",
    "k = jnp.array([[[\n",
    "    [10, 0, 0],\n",
    "    [0, 10, 0],\n",
    "    [0, 0, 10],\n",
    "    [0, 0, 10],\n",
    "]]])\n",
    "\n",
    "v = jnp.array([[[\n",
    "    [11, 0, 0],\n",
    "    [12, 0, 0],\n",
    "    [100, 5, 0],\n",
    "    [1000, 6, 0],\n",
    "]]])\n",
    "\n",
    "# The first ([10, 0, 0]) and second ([0, 10, 0]) row of `query` align with the\n",
    "# first and second `key` rows respectively, so the first and second `value`\n",
    "# vectors are returned in order as the output. This equivalently means that the\n",
    "# attention is focussed for those value's indices.\n",
    "q = jnp.array([[[\n",
    "    [10, 0, 0],\n",
    "    [0, 10, 0],\n",
    "]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "output, weight = MultiHeadedAttention.dot_attention(q, k, v)\n",
    "print(f'output:\\n{output}')\n",
    "print(f'shape: {output.shape}')\n",
    "print(f'weight:\\n{weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12"
   },
   "source": [
    "Our query consists of two vectors $[10, 0, 0]$ and $[0, 10, 0]$ respectively, which correspond to the first two rows of our key matrix. If we look at the attention weights calculated and multiply these with our value matrix, you can see that we are selecting the first 2 rows of the value matrix:\n",
    "\n",
    ">   ```[11, 0, 0]```\n",
    "\n",
    ">   ```[12, 0, 0]```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13"
   },
   "source": [
    "You will also notice that the matrix is 3-dimensional array / tensor. The dot-attention static method is designed to be used directly in the layer that takes as input a 3D tensor of shape `[batch_size, sequence_length, vector_dimension]`.\n",
    "\n",
    "Also note in Pax/JAX we use ndarray and tensor (`JTensor`) interchangeably. `JTensor` is just a typedef to `jnp.ndarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14"
   },
   "source": [
    "**Interlude #1 - Pax layers**\n",
    "\n",
    "Pax layers are a way you add structure to your code, and create new layers for yourself and hopefully for others to re-use. A layer typically represents a sub part of your network. As with any mature framework the Pax codebase implements a pletora of basic layers that you can combine into interesting ways to build the next breakthrough in AI, and who knows maybe one day approach AGI!\n",
    "\n",
    "In this example, we are building a MultiHeadedAttention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15"
   },
   "source": [
    "To build a basic functional layer you need to implement 3 code blocks\n",
    "\n",
    "```\n",
    "  class attributes to set with fiddle.\n",
    "```\n",
    "Every layer needs parameters to configure your model.\n",
    "```\n",
    "  def setup(self):\n",
    "```\n",
    "Creates the layer variables and/or sub-layers. `layer.create_child` and `layers.create_children` will be your friends for making complex networks.\n",
    "```\n",
    "  def __call__(....)\n",
    "```\n",
    "Implements the forward propagation step of your layer (mini network). In the `__call__()` step we implement the math behind our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16"
   },
   "source": [
    "**Interlude #2 - Back to MultiHeaded Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18"
   },
   "source": [
    "Let's get back to our layer above. You will notice it does much more than the static class method called dot_attention(). Multi-head attention consists of four parts:\n",
    "\n",
    "* The three inputs: $Q$ (query), $K$ (key), $V$ (value). \n",
    "* Linear layers for the inputs.\n",
    "* Scaled dot-product attention.\n",
    "* Final linear layer.\n",
    "\n",
    "In Multi-headed attention, ($K$,$Q$,$V$) are passed through seperate linear (Dense) layers for each attention head. For simplicity/efficiency the code below implements this using a single dense layer with `num_head` times as many outputs. So, essentially each head has an attention dimension of `d_model // num_head`. The output is rearranged to a shape of `(batch, num_head, ...)` before applying the attention function. Formally, multi-head attention is defined as below in terms of scaled dot-product attention:\n",
    "\n",
    "\\begin{align}\n",
    "MultiHeadAttention(Q, K, V) = Concat(head_1, ..., head_n) W^O\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\end{align}\n",
    "\n",
    "An appropriate mask must be used in the attention step. The attention output for each head is then concatenated (using `jnp.transpose`, and `jnp.reshape`) and put through a final Dense layer.\n",
    "\n",
    "\n",
    "Instead of one single attention head, $Q$, $K$, and $V$ are split into multiple heads because it allows the model to jointly attend to information from different representation subspaces at different positions. After the split each head has a reduced dimensionality ($W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$ and $d_k = d_v = \\frac{d_{model}}{num\\_head}$) so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19"
   },
   "source": [
    "**Homework #1 - jnp.einsum**\n",
    "\n",
    "Einsum is your friend. Try it below on the $q$, $k$, $v$ matrices defined above. Also take a look at a short tutorial: https://ajcr.net/Basic-guide-to-einsum/, it is quite powerful/expressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20"
   },
   "outputs": [],
   "source": [
    "# type in your own einsum code to make sure you understand einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21"
   },
   "source": [
    "**Interlude #3 - Masking**\n",
    "\n",
    "When working on TPUs you quickly learn about **padding**. When we send training data to the TPU, it really likes everything to be of the same length, but our data, more often than not, is of varied length. The solution to this is to pad our input sequences with 0's.\n",
    "\n",
    "```\n",
    "a = [1, 7, 9, 12, 3, 0, 0, 0]\n",
    "```\n",
    "\n",
    "But now if we just use this vector as input, we are confusing our model. Are the 0's data or not? This is where padding comes in. A *padding mask* will be used to represent which datapoints are real and which are padded. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "So given our example vector above, the padding mask would look as follows\n",
    "\n",
    "```\n",
    "padding_a = [0, 0, 0, 0, 0, 1, 1, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: NpTensor) -> NpTensor:\n",
    "  seq = np.equal(seq, 0).astype(np.float32)\n",
    "  return seq  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23"
   },
   "source": [
    "Let's test our padding mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24"
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 7, 9, 10, 12, 7, 0, 0, 0, 0])\n",
    "create_padding_mask(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25"
   },
   "source": [
    "Things get more interesting when we start building our Transformer translator. During training we have two inputs to our model: the source and the target sentence.\n",
    "\n",
    "I'm skipping ahead a bit here. The transformer decoder is an auto-regressive model. During inference it predicts one word at a time and then uses those predictions and the source language sentence to predict the next word. Given this, during attention modeling we need to ignore future tokens in the target sentence. To do that, we create a `look_ahead_mask` that will be used during the attention calculation to mask future tokens at any given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size: int) -> NpTensor:\n",
    "  mask = np.ones([size, size])\n",
    "  mask[np.tril_indices(size)] = 0.0\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27"
   },
   "outputs": [],
   "source": [
    "create_look_ahead_mask(a.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28"
   },
   "source": [
    "**Homework #2**\n",
    "\n",
    "The code above used `np.tril_indices()`, a pretty cool function to get the lower triangular elements in a matrix. This does not exist in JAX numpy. How can you create a lower triangular matrix of 0's and upper triangular matrix of 1's in another way. (Hint: 1<2, 2<3, 3<4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29"
   },
   "outputs": [],
   "source": [
    "# create a lower triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30"
   },
   "source": [
    "Now we have our 2 important masks, but we are not quite done yet. Notice we created a look-ahead mask for the complete sequence of our vector **a**. But the last four elements are not to be used. So we have to combined the upper triangular matrix of 1's with the padding mask we just learned about. The single line below in our final `create_masks()` function does this for us\n",
    "\n",
    "```\n",
    "np.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "```\n",
    "\n",
    "Pretty cool. One of the lessons of learning transformers, is also learning these cute tricks. Of course, we don't really need to go through all this trouble. Pax has nice helper methods for us to accomplish the same. It is good though to do some things yourself to build a deeper understanding of the modeling architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp: NpTensor,\n",
    "                 tar: NpTensor) -> tuple:\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by\n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "  # np.maximum keep the band-part (upper triangular) section for the\n",
    "  # parts of the sequence that has real data and uses a padding mask\n",
    "  # for the rest. For example, for a target sequence with length=3 padded to\n",
    "  # length=5:\n",
    "  # 0 1 1 1 1\n",
    "  # 0 0 1 1 1\n",
    "  # 0 0 0 1 1\n",
    "  # 0 0 0 1 1\n",
    "  look_ahead_mask = np.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "  return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32"
   },
   "outputs": [],
   "source": [
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(a, a)\n",
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33"
   },
   "source": [
    "# A 'special' FeedForward layer\n",
    "\n",
    "A basic building block in the Transformer architecture is a 2-layer Fully Connected FeedForward layer with RELU activations in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34"
   },
   "source": [
    "**Homework #3** - Make a drawing of this layer by inspecting the code below\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    "Add your picture above in the text box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35"
   },
   "outputs": [],
   "source": [
    "class TransformerFeedForwardLayer(base_layer.BaseLayer):\n",
    "  \"\"\"Transformer feedforward layer with residual connection and dropout.\n",
    "\n",
    "  Attributes:\n",
    "    input_dim: Depth of the input.\n",
    "    hidden_dim: Hidden dimension of FFN.\n",
    "    residual_dropout_prob: Residual dropout.\n",
    "    relu_dropout_prob: FFN dropout.\n",
    "  \"\"\"\n",
    "  input_dim: int = 0\n",
    "  hidden_dim: int = 0\n",
    "  residual_dropout_prob: float = 0.0\n",
    "  relu_dropout_prob: float = 0.0\n",
    "\n",
    "  def setup(self) -> None:\n",
    "    self.create_child(\n",
    "        'layer_norm',\n",
    "        pax_fiddle.Config(\n",
    "            layers.LayerNorm, dim=self.input_dim))\n",
    "\n",
    "    # Create the first Feedforward layer, mapping to hidden dim.\n",
    "    self.create_child(\n",
    "        'ffn_layer1',\n",
    "        pax_fiddle.Config(\n",
    "            layers.FeedForward, input_dims=self.input_dim,\n",
    "            output_dims=self.hidden_dim,\n",
    "            activation_tpl=pax_fiddle.Config(layers.ReLU)))\n",
    "\n",
    "    # Create RELU dropout layer.\n",
    "    self.create_child(\n",
    "        'relu_dropout',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Dropout, keep_prob=1.0 - self.relu_dropout_prob))\n",
    "\n",
    "    # Create the second Feedforward layer, mapping from hidden dim.\n",
    "    self.create_child(\n",
    "        'ffn_layer2',\n",
    "        pax_fiddle.Config(\n",
    "            layers.FeedForward, input_dims=self.hidden_dim,\n",
    "            output_dims=self.input_dim,\n",
    "            activation_tpl=pax_fiddle.Config(layers.Identity)))\n",
    "\n",
    "    # Create residual dropout layer.\n",
    "    self.create_child(\n",
    "        'residual_dropout',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Dropout, keep_prob=1.0 - self.residual_dropout_prob))\n",
    "\n",
    "  def __call__(self,\n",
    "            inputs: JTensor,\n",
    "            paddings: Optional[JTensor] = None) -> JTensor:\n",
    "    \"\"\"Computes transformer feedforward layer outputs given inputs and paddings.\n",
    "\n",
    "    Args:\n",
    "      inputs: JTensor of shape [B, T, D].\n",
    "      paddings: JTensor of shape [B, T]. A binary tensor where 1 indicates a\n",
    "        padded position (should be skipped).\n",
    "\n",
    "    Returns:\n",
    "      JTensor of shape [B, T, D].\n",
    "    \"\"\"\n",
    "    inputs_normalized = self.layer_norm(inputs)\n",
    "    # Apply the first FFN layer.\n",
    "    projected_inputs = self.ffn_layer1(inputs_normalized)\n",
    "    # Apply RELU dropout.\n",
    "    projected_inputs = self.relu_dropout(projected_inputs)\n",
    "    # Apply second FFN layer.\n",
    "    projected_inputs = self.ffn_layer2(projected_inputs)\n",
    "    # Apply residual dropout.\n",
    "    residual = self.residual_dropout(projected_inputs)\n",
    "    out = inputs + residual\n",
    "    out *= (1.0 - jnp.expand_dims(paddings, axis=-1))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36"
   },
   "source": [
    "The FeedForward layer has a few key difference compared to a standard 3 layer network.\n",
    "\n",
    "* Inputs are immediately normalized (LayerNorm)\n",
    "* We apply a Dropout on the final projected output\n",
    "* Finally, we add a skip connection from input to the output. This helps with passing gradients down deep networks by improving the stability of error backpropagation across many stacked feed-forward layers. The discussions in the conclusion of [Understanding the Difficulty of Training Deep Feedforward Neural Networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), as well as Section 2 of [Highway Networks](https://arxiv.org/abs/1505.00387) are good starting points for developing an intuition around how this happens. We note the importance of parameter initialization for the feed-forward block alongside the skip connection, with the goal of keeping the Jacobian around 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37"
   },
   "source": [
    "We have now defined two new layers already that form the basic building blocks for our Transformer model below. But before we get to the really interesting part of Transformers, let's build and test our layer above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38"
   },
   "outputs": [],
   "source": [
    "b = 2\n",
    "input_dim = 20\n",
    "\n",
    "ff_p = pax_fiddle.Config(\n",
    "    TransformerFeedForwardLayer, \n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=100,\n",
    "    residual_dropout_prob=0.1,\n",
    "    relu_dropout_prob=0.1,\n",
    "    name=\"ff_net\")\n",
    "ff = instantiate(ff_p)\n",
    "\n",
    "with base_layer.JaxContext.new_context():\n",
    "  key, init_key, random_key = jax.random.split(key, 3)\n",
    "  rngs = {PARAMS: init_key, RANDOM: random_key}\n",
    "  ff_vars = ff.init(\n",
    "      rngs,\n",
    "      jnp.zeros([b, 4, input_dim]),\n",
    "      jnp.zeros([b, 4]),\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39"
   },
   "source": [
    "Let's inspect our layers variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40"
   },
   "outputs": [],
   "source": [
    "ff_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41"
   },
   "source": [
    "Let's test our FeedForward network on some randomly generated vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42"
   },
   "outputs": [],
   "source": [
    "# Here we are creating a random sequence input with batch_size=2,\n",
    "# sequence_length=35 and input_dim=20.\n",
    "b, s, d = 2, 35, 20\n",
    "key, k1, k2 = jax.random.split(key, 3)\n",
    "vec = jax.random.uniform(k1, [b, s, d])\n",
    "paddings = jnp.concatenate((jnp.zeros([b, 20]), jnp.ones([b, 15])), axis=1)\n",
    "\n",
    "with base_layer.JaxContext.new_context():\n",
    "  out = ff.apply(ff_vars, vec, paddings, rngs={RANDOM: k2})\n",
    "\n",
    "print(f'out: {out.shape}')\n",
    "print(f'out: {out[0][0]}')\n",
    "print(f'out: {out[0][20]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43"
   },
   "source": [
    "A few things are happening here. To test our layer, we need to run it as part of a JaxContext. Many layers don't require context, but this layer uses random DropOut during training for regularization. To make Dropout deterministic between forward and backward pass we define a global context for the layer to run within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44"
   },
   "source": [
    "# A vanilla multi-headed transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46"
   },
   "source": [
    "We are now ready to build our Transformer Translator. The first building block of the Translator is the Transformer Layer. This takes as input an embedding vector, applies self attention (that is when query, key and value are all the same) and then normalizes the output before feeding it into the FeedForward layer we just built above. Note as with the FeedForward layer we use skip connections, which helps with gradient-descent through very deep networks.\n",
    "\n",
    "This forms the basics of a Transformer Encoder.\n",
    "\n",
    "When we build a Translation model we also want to learn the correlations between the target language and the source language. This is accomplished using Cross-Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(base_layer.BaseLayer):\n",
    "  \"\"\"Transformer layer with multi-headed attention.\n",
    "\n",
    "  Attributes:\n",
    "    model_dim: Dimension of the transformer block input.\n",
    "    hidden_dim: Hidden dimension of FFN layer.\n",
    "    num_head: Num of heads in self attention.\n",
    "    atten_dropout_prob: Probability at which we apply dropout to the attention\n",
    "      weights.\n",
    "    residual_dropout_prob: Probability at which we apply dropout to the\n",
    "      residual layers, such that, residual(x, y) = (x + dropout(y)).\n",
    "    relu_dropout_prob: Probability at which we apply dropout to the FFN\n",
    "      layers.\n",
    "    cross_attention_tpl: If True, perform cross encoder-decoder attention.\n",
    "  \"\"\"\n",
    "  model_dim: int = 0\n",
    "  hidden_dim: int = 0\n",
    "  num_head: int = 0\n",
    "  atten_dropout_prob: float = 0.0\n",
    "  residual_dropout_prob: float = 0.0\n",
    "  relu_dropout_prob: float = 0.0\n",
    "  # NOTE: the name of this attribute cannot be the same as the\n",
    "  #       name created in create_child in setup below (e.g., also cross-attention),\n",
    "  #       otherwise, you will get an error\n",
    "  cross_attention_tpl: bool = False\n",
    "\n",
    "  def setup(self) -> None:\n",
    "    # Layer norm before the self-attention layer.\n",
    "    self.create_child(\n",
    "        'self_atten_ln',\n",
    "        pax_fiddle.Config(layers.LayerNorm, dim=self.model_dim))\n",
    "    # Multi-headed self-attention\n",
    "    self.create_child(\n",
    "        'self_attention',\n",
    "        pax_fiddle.Config(\n",
    "            MultiHeadedAttention,\n",
    "            input_dim=self.model_dim,\n",
    "            num_head=self.num_head,\n",
    "            atten_dropout_prob=self.atten_dropout_prob))\n",
    "    # Dropout to apply to the residual path for regularization.\n",
    "    self.create_child(\n",
    "        'residual_dropout',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Dropout, keep_prob=1.0-self.residual_dropout_prob))\n",
    "    # Multi-headed cross-attention\n",
    "    if self.cross_attention_tpl:\n",
    "      self.create_child(\n",
    "          'cross_atten_ln',\n",
    "          pax_fiddle.Config(layers.LayerNorm, dim=self.model_dim))\n",
    "      self.create_child(\n",
    "          'cross_attention',\n",
    "          pax_fiddle.Config(\n",
    "              MultiHeadedAttention,\n",
    "              input_dim=self.model_dim,\n",
    "              num_head=self.num_head,\n",
    "              atten_dropout_prob=self.atten_dropout_prob))\n",
    "    # Feed-forward layer, which has already incorporated layer norm and\n",
    "    # residual path.\n",
    "    self.create_child(\n",
    "        'ff_layer',\n",
    "        pax_fiddle.Config(\n",
    "            TransformerFeedForwardLayer,\n",
    "            input_dim=self.model_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            relu_dropout_prob=self.relu_dropout_prob,\n",
    "            residual_dropout_prob=self.residual_dropout_prob))\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      inputs: JTensor,\n",
    "      paddings: JTensor,\n",
    "      atten_mask: JTensor,\n",
    "      cross_inputs: Optional[JTensor] = None,\n",
    "      cross_atten_mask: Optional[JTensor] = None) -> tuple:\n",
    "    \"\"\"Transformer layer.\n",
    "\n",
    "    Args:\n",
    "      inputs: Input sequence JTensor of shape [B, T, H].\n",
    "      paddings: Input paddings JTensor of shape [B, T] (only used in FFN layer).\n",
    "      atten_mask: Self attention mask ready to add to the logits. It can be of\n",
    "        shape [B/1, 1, T/1, T] which is broadcast compatible with the self\n",
    "        attention matrix of shape [B, N, T, T]. This is assumed to have combined\n",
    "        paddings, causal masking as well as segment maskings.\n",
    "      cross_inputs: Output of the encoder, to be used for cross attention, of\n",
    "        shape [B, S, H].\n",
    "      cross_atten_mask: Cross attention mask ready to add to the logits. It can\n",
    "        be of shape [B/1, 1, T/1, S] which is broadcast compatible with the\n",
    "        cross attention matrix of shape [B, N, T, S]. This is assumed to have\n",
    "        combined paddings as well as segment maskings.\n",
    "\n",
    "    Returns:\n",
    "      The fflayer output with shape [B, T, D].\n",
    "      atten_probs: A NestedMap with keys `self_atten` <float>[B, N, T, T].\n",
    "    \"\"\"\n",
    "    # Layer normalize input\n",
    "    inputs_normalized = self.self_atten_ln(inputs)\n",
    "\n",
    "    # Compute self-attention, query/key/value vectors are the input itself\n",
    "    atten_output, _ = self.self_attention(\n",
    "        inputs_normalized,\n",
    "        inputs_normalized,\n",
    "        inputs_normalized,\n",
    "        atten_mask=atten_mask)\n",
    "\n",
    "    # Residual dropout and connection\n",
    "    atten_output = self.residual_dropout(atten_output)\n",
    "    atten_output += inputs\n",
    "\n",
    "    # Apply cross attention if applicable\n",
    "    if self.cross_attention_tpl:\n",
    "      assert cross_inputs is not None\n",
    "      assert cross_atten_mask is not None\n",
    "      cross_atten_query = self.cross_atten_ln(atten_output)\n",
    "      cross_atten_output, _ = self.cross_attention(\n",
    "          cross_atten_query,\n",
    "          cross_inputs,\n",
    "          cross_inputs,\n",
    "          atten_mask=cross_atten_mask)\n",
    "\n",
    "      # Residual dropout and connection\n",
    "      cross_atten_output = self.residual_dropout(cross_atten_output)\n",
    "      atten_output += cross_atten_output\n",
    "\n",
    "    else:\n",
    "      # Ensure cross inputs are not passed in to a layer that does not support\n",
    "      # cross-attention (otherwise we would silently ignore the inputs).\n",
    "      assert cross_inputs is None\n",
    "      assert cross_atten_mask is None\n",
    "\n",
    "    # Apply FFN layer\n",
    "    output = self.ff_layer(atten_output, paddings=paddings)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48"
   },
   "source": [
    "# Translation encoder / decoder (building from above)\n",
    "\n",
    "Our Transformer Translator consists of an Encoder (N stacked Transformer Layers) and a Decoder that uses cross attention with the output of the Encoder block as the cross attention inputs.\n",
    "\n",
    "Let's zoom into the Encoder block, implemented below as:\n",
    "```\n",
    "  def encoder_fprop(self, seq: JTensor, paddings: JTensor) -> tuple[JTensor, JTensor]:\n",
    "\n",
    "```\n",
    "The encoder module takes as input a batch of sequences, and their corresponding paddings. To connect these sequence to the stack of Transformer layers we compute an input embedding from the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49"
   },
   "source": [
    "**Interlude - Word Embeddings**\n",
    "\n",
    "[Word embeddings](https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81#:~:text=Word%20Embedding%20%3D%3E%20Collective%20term%20for,to%20learn%20from%20text%20data) are the basis of any language model.\n",
    "\n",
    "The word embedding basically takes the raw index of the word and maps it into a high dimensional (embedding) space. In our Transformer Translator, we use the `Embedding` (and `SharedEmbeddingSoftmax` for the decoder since we perform a softmax to get output token probabilities) layer to compute the embedding vector for us.\n",
    "\n",
    "In the following example, we first tokenize our input `\"word embedding\"` by words, get a sparse representation, and perform an embedding lookup from our tiny vocabulary we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50"
   },
   "outputs": [],
   "source": [
    "input_sentence = 'word embedding'\n",
    "vocabulary = {'word': 1, 'embedding': 2, 'vectors': 3, 'OOV': 0}\n",
    "input_ids = [\n",
    "    vocabulary.get(word, vocabulary['OOV']) for word in input_sentence.split()\n",
    "]\n",
    "\n",
    "emb_p = pax_fiddle.Config(\n",
    "    layers.Embedding,\n",
    "    num_classes=len(vocabulary),\n",
    "    input_dims=3,  # embedding output size\n",
    "    name='embedding_example',\n",
    ")\n",
    "emb = instantiate(emb_p)\n",
    "\n",
    "with base_layer.JaxContext.new_context():\n",
    "  key, emb_key = jax.random.split(key, 2)\n",
    "  emb_vars = emb.init(\n",
    "      emb_key, np.zeros([2, 2], dtype=jnp.int16), method=emb.emb_lookup)\n",
    "\n",
    "# Now let's lookup the dense embedding representations for the word 'vectors'\n",
    "word_embeddings = emb.apply(emb_vars, input_ids, method=emb.emb_lookup)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51"
   },
   "source": [
    "**Interlude - Positional Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52"
   },
   "source": [
    "Attention layers see their input as a set of vectors, with no sequential order. This model also doesn't contain any recurrent or convolutional layers. Because of this, a \"positional encoding\" is added to give the model some information about the relative position of the tokens in the sentence.\n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of tokens in a sentence. So after adding the positional encoding, tokens will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space.\n",
    "\n",
    "For a more detailed description of positional encoding, I found the following blog [Mastering positional encoding](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3) incredibly insightful. A fun read for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos: NpTensor, i: NpTensor, d_model: int) -> NpTensor:\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position: int, d_model: int) -> NpTensor:\n",
    "  angle_rads = get_angles(\n",
    "      np.arange(position)[:, np.newaxis],\n",
    "      np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "  return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n, d = 2048, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]\n",
    "\n",
    "# Juggle the dimensions for the plot\n",
    "pos_encoding = np.reshape(pos_encoding, (n, d // 2, 2))\n",
    "pos_encoding = np.transpose(pos_encoding, (2, 1, 0))\n",
    "pos_encoding = np.reshape(pos_encoding, (d, n))\n",
    "\n",
    "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56"
   },
   "source": [
    "In the code, below we use the Pax layer PositionalEmbedding to accomplish the above for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57"
   },
   "outputs": [],
   "source": [
    "class Translator(base_layer.BaseLayer):\n",
    "  \"\"\"Transformer layer for translation composed of stacked TransformerLayers.\n",
    "\n",
    "  Attributes:\n",
    "    num_encoder_layer: Number of encoder transformer layers.\n",
    "    num_decoder_layer: Number of decoder transformer layers.\n",
    "    model_dim: Embeddding dimension.\n",
    "    num_head: Number of attention heads.\n",
    "    hidden_dim: Feedforward layer hidden dim.\n",
    "    encoder_vocab_size: Input vocabulary size.\n",
    "    decoder_vocab_size: Output target vocabulary size.\n",
    "    dropout_prob: Dropout rate.\n",
    "  \"\"\"\n",
    "  num_encoder_layer: int = 0\n",
    "  num_decoder_layer: int = 0\n",
    "  model_dim: int = 0\n",
    "  num_head: int = 0\n",
    "  hidden_dim: int = 0\n",
    "  encoder_vocab_size: int = 0\n",
    "  decoder_vocab_size: int = 0\n",
    "  dropout_prob: float = 0.0\n",
    "\n",
    "  def setup(self) -> None:\n",
    "\n",
    "    def _LayerConfig(ii: int, prefix: str,\n",
    "                     cross_atten: bool) -> pax_fiddle.Config:\n",
    "      \"\"\"Construct ii-th layer config.\"\"\"\n",
    "      return pax_fiddle.Config(\n",
    "          TransformerLayer,\n",
    "          name = f'{prefix}layer_{ii}',\n",
    "          num_head = self.num_head,\n",
    "          model_dim = self.model_dim,\n",
    "          hidden_dim = self.hidden_dim,\n",
    "          atten_dropout_prob = self.dropout_prob,\n",
    "          relu_dropout_prob = self.dropout_prob,\n",
    "          residual_dropout_prob = self.dropout_prob,\n",
    "          cross_attention_tpl = cross_atten)\n",
    "\n",
    "    # Positional encoding\n",
    "    self.create_child(\n",
    "        'position_emb',\n",
    "        pax_fiddle.Config(\n",
    "            layers.PositionalEmbedding, embedding_dims=self.model_dim))\n",
    "\n",
    "    # Encoder\n",
    "    self.create_child(\n",
    "        'enc_dropout',\n",
    "        pax_fiddle.Config(layers.Dropout, keep_prob=1.0 - self.dropout_prob))\n",
    "    self.create_child(\n",
    "        'enc_embedding',\n",
    "        pax_fiddle.Config(\n",
    "            layers.Embedding, input_dims=self.model_dim,\n",
    "            num_classes=self.encoder_vocab_size))\n",
    "    self.create_child(\n",
    "        'enc_ln',\n",
    "        pax_fiddle.Config(layers.LayerNorm, dim=self.model_dim))\n",
    "    self.create_children('enc_layer', [\n",
    "        _LayerConfig(ii, 'enc_', cross_atten=False)\n",
    "        for ii in range(self.num_encoder_layer)\n",
    "    ])\n",
    "\n",
    "    # Decoder\n",
    "    self.create_child(\n",
    "        'dec_dropout',\n",
    "        pax_fiddle.Config(layers.Dropout, keep_prob=1.0 - self.dropout_prob))\n",
    "    self.create_child(\n",
    "        'dec_embedding',\n",
    "        pax_fiddle.Config(\n",
    "            layers.SharedEmbeddingSoftmax, input_dims=self.model_dim,\n",
    "            num_classes=self.decoder_vocab_size))\n",
    "    self.create_child(\n",
    "        'dec_ln',\n",
    "        pax_fiddle.Config(layers.LayerNorm, dim=self.model_dim))\n",
    "    self.create_children('dec_layer', [\n",
    "        _LayerConfig(ii, 'dec_', cross_atten=True)\n",
    "        for ii in range(self.num_decoder_layer)\n",
    "    ])\n",
    "\n",
    "  def encoder_fprop(self, seq: JTensor,\n",
    "                    paddings: JTensor) -> tuple:\n",
    "    # seq.shape = [batch_size, seq_len]\n",
    "    b, s = seq.shape\n",
    "    base_layer.assert_has_shape(paddings, [b, s])\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    input_emb = self.enc_embedding.emb_lookup(seq)\n",
    "    position_emb = self.position_emb(seq_length=s)\n",
    "    base_layer.assert_has_shape(input_emb, [b, s, self.model_dim])\n",
    "    base_layer.assert_has_shape(position_emb, [1, s, self.model_dim])\n",
    "    x = input_emb + position_emb\n",
    "    x = self.enc_dropout(x)\n",
    "\n",
    "    # compute attention mask to be applied to the logits\n",
    "    atten_mask, _ = layers.compute_attention_masks_for_fprop(\n",
    "        x, paddings, causal_attention=False)\n",
    "\n",
    "    # compute n stacks of self attention\n",
    "    for i in range(self.num_encoder_layer):\n",
    "      x = self.enc_layer[i](x, paddings, atten_mask)\n",
    "\n",
    "    # final layer norm\n",
    "    x = self.enc_ln(x)\n",
    "\n",
    "    base_layer.assert_has_shape(x, [b, s, self.model_dim])\n",
    "    return x  # (batch_size, seq_len, d_model)\n",
    "\n",
    "  def decoder_fprop(self, tar: JTensor, tar_paddings: JTensor,\n",
    "                    enc_output: JTensor, enc_paddings: JTensor) -> JTensor:\n",
    "    # tar.shape = [batch_size, seq_len]\n",
    "    b, s = tar.shape\n",
    "    base_layer.assert_has_shape(tar_paddings, [b, s])\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    input_emb = self.dec_embedding.emb_lookup(tar)\n",
    "    position_emb = self.position_emb(seq_length=s)\n",
    "    x = input_emb + position_emb\n",
    "    x = self.dec_dropout(x)\n",
    "\n",
    "    # compute attention mask to be applied to the logits\n",
    "    self_atten_mask, cross_atten_mask = (\n",
    "        layers.compute_attention_masks_for_fprop(\n",
    "            x,\n",
    "            tar_paddings,\n",
    "            causal_attention=True,\n",
    "            cross_inputs=enc_output,\n",
    "            cross_paddings=enc_paddings))\n",
    "\n",
    "    # compute n stacks of self attention\n",
    "    for i in range(self.num_decoder_layer):\n",
    "      x = self.dec_layer[i](\n",
    "          inputs=x,\n",
    "          paddings=tar_paddings,\n",
    "          atten_mask=self_atten_mask,\n",
    "          cross_inputs=enc_output,\n",
    "          cross_atten_mask=cross_atten_mask)\n",
    "\n",
    "    # final layer norm\n",
    "    x = self.dec_ln(x)\n",
    "\n",
    "    base_layer.assert_has_shape(x, [b, s, self.model_dim])\n",
    "    return x  # (batch_size, tar_len, d_model)\n",
    "\n",
    "  def decoder_softmax(self, xformer_output: JTensor, label_ids: JTensor,\n",
    "                      paddings: JTensor) -> NestedMap:\n",
    "    label_weights = (1.0 - paddings)\n",
    "    xent_output = self.dec_embedding(\n",
    "        xformer_output,\n",
    "        class_weights=label_weights[:, :, jnp.newaxis],\n",
    "        class_ids=label_ids[:, :, jnp.newaxis])\n",
    "    return xent_output\n",
    "\n",
    "  def __call__(self, inputs: NestedMap) -> NestedMap:\n",
    "    encoder_out = self.encoder_fprop(inputs.input_ids, inputs.input_paddings)\n",
    "    decoder_out = self.decoder_fprop(inputs.target_ids, inputs.target_paddings,\n",
    "                                     encoder_out, inputs.input_paddings)\n",
    "    xent_output = self.decoder_softmax(decoder_out, inputs.target_labels,\n",
    "                                       inputs.target_paddings)\n",
    "    return xent_output\n",
    "\n",
    "  def compute_loss(self, xent_output: NestedMap, label_ids: JTensor,\n",
    "                   paddings: JTensor) -> dict:\n",
    "    label_weights = (1.0 - paddings)\n",
    "\n",
    "    num_preds = jnp.sum(label_weights)\n",
    "    per_token_xent = xent_output.per_example_xent * label_weights\n",
    "    avg_xent = jnp.sum(per_token_xent) / jnp.maximum(jnp.sum(label_weights), 1)\n",
    "    predicted_labels = xent_output.per_example_argmax\n",
    "    mean_acc = (\n",
    "        jnp.sum((label_ids == predicted_labels) * label_weights) /\n",
    "        jnp.maximum(num_preds, 1))\n",
    "\n",
    "    return {\n",
    "        'avg_xent': (avg_xent, num_preds),\n",
    "        'mean_acc': (mean_acc, num_preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58"
   },
   "source": [
    "**Testing the Encoder**\n",
    "\n",
    "While developing the Transformer Translator, I explicitly broke the `fprop` function into sub parts so that I could test each separately. Here we will test the encoder with some random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59"
   },
   "outputs": [],
   "source": [
    "# Test out the Transformer layer just created.\n",
    "input_dim = 8\n",
    "src_seq_len = 12\n",
    "batch_size = 2\n",
    "translator_p = pax_fiddle.Config(\n",
    "    Translator,\n",
    "    name='xformer',\n",
    "    model_dim=input_dim,\n",
    "    num_encoder_layer=2,\n",
    "    num_decoder_layer=2,\n",
    "    num_head=2,\n",
    "    hidden_dim=4 * input_dim,\n",
    "    dropout_prob=0.1,\n",
    "    encoder_vocab_size=8000,\n",
    "    decoder_vocab_size=8000)\n",
    "translator = instantiate(translator_p)\n",
    "\n",
    "# Initializes model weights\n",
    "\n",
    "with base_layer.JaxContext.new_context():\n",
    "  key, trans_key, random_key = jax.random.split(key, 3)\n",
    "  rngs = {PARAMS: trans_key, RANDOM: random_key}\n",
    "\n",
    "  dummy_inputs = NestedMap(\n",
    "      input_ids=jnp.ones([batch_size, input_dim], dtype=jnp.int16),\n",
    "      input_paddings=jnp.ones([batch_size, input_dim], dtype=jnp.int16),\n",
    "      target_ids=jnp.ones([batch_size, input_dim], dtype=jnp.int16),\n",
    "      target_paddings=jnp.ones([batch_size, input_dim], dtype=jnp.int16),\n",
    "      target_labels=jnp.ones([batch_size, input_dim], dtype=jnp.int16),\n",
    "  )\n",
    "  translator_vars = translator.init(rngs, dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60"
   },
   "outputs": [],
   "source": [
    "input_seq = np.random.randint(1, 200, (2, 20))\n",
    "input_seq = np.concatenate((input_seq, np.zeros([2, 15], dtype=input_seq.dtype)), axis=1)\n",
    "input_paddings = create_padding_mask(input_seq)\n",
    "input_seq, input_paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61"
   },
   "outputs": [],
   "source": [
    "# Setup mock model inputs.\n",
    "key, k3 = jax.random.split(key)\n",
    "with base_layer.JaxContext.new_context():\n",
    "  encoder_out = translator.apply(\n",
    "      translator_vars,\n",
    "      input_seq,\n",
    "      input_paddings,\n",
    "      rngs={RANDOM: k3},\n",
    "      method=translator.encoder_fprop)\n",
    "\n",
    "# Print out the output for inspection.\n",
    "print(f'encoder_out:\\n{encoder_out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62"
   },
   "source": [
    "**Homework**\n",
    "\n",
    "Test the full encoder/decoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63"
   },
   "outputs": [],
   "source": [
    "# Create a random target sequence\n",
    "\n",
    "# Right shift the target sequence - this is needed to compute the loss of the output Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64"
   },
   "source": [
    "# Loading real data from pt-en translation task\n",
    "\n",
    "We are getting close to training our Transformer Translator on real data. In this example, we will train a translation model from Portugese (pt) to English (en). To load and process the data, we use supporting libraries from the tensorflow codebase.\n",
    "\n",
    "Pax has a wrapper around datasets as well, which I will add to the tutorial at some point. For now we will be loading the data and processing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66"
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load(\n",
    "    'ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "train_examples = examples['train']\n",
    "val_examples = examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67"
   },
   "outputs": [],
   "source": [
    "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
    "cache_dir = tempfile.mkdtemp()\n",
    "tf.keras.utils.get_file(\n",
    "    f\"{model_name}.zip\",\n",
    "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
    "    cache_dir=cache_dir,\n",
    "    cache_subdir=\"\",\n",
    "    extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68"
   },
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(os.path.join(cache_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69"
   },
   "outputs": [],
   "source": [
    "def tokenize_pairs(pt: tf.Tensor, en: tf.Tensor) -> tuple:\n",
    "  pt = tokenizers.pt.tokenize(pt)\n",
    "  # Convert from ragged to dense, padding with zeros.\n",
    "  pt = pt.to_tensor()\n",
    "\n",
    "  en = tokenizers.en.tokenize(en)\n",
    "  # Convert from ragged to dense, padding with zeros.\n",
    "  en = en.to_tensor()\n",
    "\n",
    "  return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71"
   },
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "print(f'num_devices: {num_devices}')\n",
    "print(f'device kind: {jax.local_devices()[0].device_kind}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72"
   },
   "source": [
    "**Prep batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73"
   },
   "outputs": [],
   "source": [
    "# This is per-core batch size. Total batch size is BATCH_SIZE * num_devices.\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE * num_devices)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    "      .as_numpy_iterator())\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74"
   },
   "source": [
    "**Padding - to the extreme**\n",
    "\n",
    "Every batch is padded already by the TF dataset iterator. However, when we run the data on TPU below, every time the batch size changes, the XLA compiler will recompile everything. This makes training really slow. To avoid this, we pad to the maximum sequence length in the corpus (computed by first iterating over the corpus).\n",
    "\n",
    "There are more intelligent ways to work with large corpora by bucketing into a set of sequence lengths, but for the purpose of the tutorial we will just pad. The TPUs computation on all those zeroes is still much, much faster than the recompilation that would happen if we don't pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75"
   },
   "outputs": [],
   "source": [
    "max_inp_len = 291  # max len of sequences\n",
    "max_tar_len = 320\n",
    "\n",
    "\n",
    "def prepare_input_batch(input_ids: NpTensor, target_ids: NpTensor) -> NestedMap:\n",
    "  \"\"\"Prepares a input batch to feed to trainer or evaler.\"\"\"\n",
    "\n",
    "  def pad_batch(inp: NpTensor, tar: NpTensor) -> tuple:\n",
    "    \"\"\"Pad inp and tar to the max sequence length.\"\"\"\n",
    "    assert inp.shape[0] == BATCH_SIZE * num_devices\n",
    "    assert inp.shape[-1] <= max_inp_len\n",
    "    assert tar.shape[-1] <= max_tar_len\n",
    "\n",
    "    inp_pad_len = max_inp_len - inp.shape[1]\n",
    "    tar_pad_len = max_tar_len - tar.shape[1]\n",
    "    inp = jnp.concatenate(\n",
    "        (inp, jnp.zeros((inp.shape[0], inp_pad_len), dtype=jnp.int32)), 1)\n",
    "    tar = jnp.concatenate(\n",
    "        (tar, jnp.zeros((tar.shape[0], tar_pad_len), dtype=jnp.int32)), 1)\n",
    "\n",
    "    inp = inp.reshape(num_devices, BATCH_SIZE, inp.shape[-1])\n",
    "    tar = tar.reshape(num_devices, BATCH_SIZE, tar.shape[-1])\n",
    "    return inp, tar\n",
    "\n",
    "  # Let's pad input_ids, target_ids to the fixed max sequence length.\n",
    "  input_ids, target_ids = pad_batch(input_ids, target_ids)\n",
    "\n",
    "  # During training, we always predict the next token.\n",
    "  target_labels = jnp.concatenate(\n",
    "      (\n",
    "          target_ids[:, :, 1:],\n",
    "          jnp.zeros_like(target_ids[:, :, :1]),\n",
    "      ),\n",
    "      2,\n",
    "  )\n",
    "\n",
    "  input_paddings = jnp.equal(input_ids, 0).astype(jnp.float32)\n",
    "  target_paddings = jnp.equal(target_ids, 0).astype(jnp.float32)\n",
    "\n",
    "  return NestedMap(\n",
    "      input_ids=input_ids,\n",
    "      input_paddings=input_paddings,\n",
    "      target_ids=target_ids,\n",
    "      target_paddings=target_paddings,\n",
    "      target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76"
   },
   "source": [
    "# Integrate with Pax Trainer using Adam Optimizer\n",
    "\n",
    "Pax makes it easy to integrate `optax` (Jax optimization libraries), using the BaseModel subclass.\n",
    "\n",
    "```\n",
    "class TranslatorModel(base_model.BaseModel):\n",
    ".\n",
    ".\n",
    "  def compute_predictions(self, input_batch):\n",
    ".\n",
    "  def compute_loss(self, xent_output, input_batch):\n",
    ".\n",
    "```\n",
    "\n",
    "Using the two methods `compute_predictions()` and `compute_loss()`, we can quickly integrate with any optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77"
   },
   "outputs": [],
   "source": [
    "from praxis import base_model\n",
    "from praxis import optimizers\n",
    "from praxis import schedules\n",
    "from paxml import tasks_lib\n",
    "from paxml import trainer_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78"
   },
   "outputs": [],
   "source": [
    "class TranslatorModel(base_model.BaseModel):\n",
    "  \"\"\"Translator Model base task.\n",
    "\n",
    "  Attributes:\n",
    "    tt_tpl: A Transformer translator.\n",
    "  \"\"\"\n",
    "  tt_tpl: pax_fiddle.Config[base_model.BaseModel] = pax_fiddle.template_field(Translator)\n",
    "  \n",
    "  def setup(self) -> None:\n",
    "    # Construct the model.\n",
    "    self.create_child('tt', self.tt_tpl)\n",
    "\n",
    "  def compute_predictions(self, input_batch: NestedMap) -> NestedMap:\n",
    "    \"\"\"Computes predictions for `input_batch`.\"\"\"\n",
    "    print('TranslatorModel.compute_predictions')\n",
    "    return self.tt(input_batch)\n",
    "\n",
    "  def compute_loss(self, xent_output: NestedMap,\n",
    "                   input_batch: NestedMap) -> tuple:\n",
    "    print('TranslatorModel.compute_loss')\n",
    "    per_example_out = NestedMap()\n",
    "    metrics = self.tt.compute_loss(xent_output, input_batch.target_labels,\n",
    "                                   input_batch.target_paddings)\n",
    "    return metrics, per_example_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79"
   },
   "source": [
    "Configure our model. Below we define the Transformer Translator, followed by the learner (Adam) and the Learning rate schedule (`LinearRampupExponentialDecay`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80"
   },
   "outputs": [],
   "source": [
    "task_p = tasks_lib.SingleTask.HParams(name='translation')\n",
    "task_p.model = pax_fiddle.Config(\n",
    "    TranslatorModel,\n",
    "    name='jax_transformer',\n",
    "    tt_tpl=pax_fiddle.Config(\n",
    "        Translator,\n",
    "        name = 'xformer',\n",
    "        model_dim = 128,\n",
    "        hidden_dim = 4 * 128,\n",
    "        num_head = 8,\n",
    "        num_encoder_layer = 4,\n",
    "        num_decoder_layer = 4,\n",
    "        encoder_vocab_size = 8500,\n",
    "        decoder_vocab_size = 8000,\n",
    "        dropout_prob = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81"
   },
   "outputs": [],
   "source": [
    "lp = task_p.train.learner\n",
    "lp.loss_name = 'avg_xent'\n",
    "lp.optimizer = optimizers.Adam.HParams(\n",
    "    beta1=0.9,\n",
    "    beta2=0.98,\n",
    "    epsilon=1e-9,\n",
    "    weight_decay=0,\n",
    "    clip_gradient_norm_to_value=5.0)\n",
    "lp.optimizer.lr_schedule = schedules.Transformer.HParams(\n",
    "    warmup_steps=4000, model_dim=128)\n",
    "lp.optimizer.learning_rate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set pmap_use_tensorstore\n",
    "from absl import flags\n",
    "flags.FLAGS.pmap_use_tensorstore = True\n",
    "flags.FLAGS['pmap_use_tensorstore'].parse(\"True\")\n",
    "\n",
    "flags.FLAGS.pmap_use_tensorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82"
   },
   "source": [
    "With everything configured, let's instantiate our model and associated training states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83"
   },
   "outputs": [],
   "source": [
    "\n",
    "jax_task = instantiate(task_p)\n",
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "# To correctly prepare a batch of data for model initialization (now that shape\n",
    "# inference is merged), we take one devices*batch_size tensor tuple of data,\n",
    "# slice out just one batch, then run the prepare_input_batch function over it.\n",
    "sample_inputs = prepare_input_batch(\n",
    "    *next(make_batches(train_examples))).Transform(lambda t: t[0, ...])\n",
    "\n",
    "jax_model_states = trainer_lib.initialize_model_state(jax_task, init_key,\n",
    "                                                      sample_inputs)\n",
    "jax_vars = jax_model_states.mdl_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84"
   },
   "source": [
    "**Interlude - Learning rate schedules**\n",
    "\n",
    "One of the interesting parts of building an ML model is how to train it. There have been quite a few advances in the last few years on various optimization algorithms. One aspect of this is the learning rate schedule. In the schedule we plot below, you can see we first 'warm-up' by ramping up the learning rate. After 4000 steps we start an exponential decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85"
   },
   "outputs": [],
   "source": [
    "steps = jnp.arange(1, 40000)\n",
    "lr = jax_task.learners[0].optimizer.get_learning_rate(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86"
   },
   "outputs": [],
   "source": [
    "plt.plot(steps, lr)\n",
    "plt.xlabel('step number')\n",
    "plt.ylabel('learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87"
   },
   "source": [
    "This notebook connects to a set of 4 v4 TPUs chips. Before we starting training on all 4 TPU's we need to replicate the model state to the TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of devices = {jax.local_device_count()}\")\n",
    "replicated_jax_states = trainer_lib.replicate_model_state(jax_model_states)\n",
    "replicated_jax_vars = replicated_jax_states.mdl_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89"
   },
   "outputs": [],
   "source": [
    "def train_step(states, prng_key, inputs):\n",
    "  return trainer_lib.train_step_single_learner(jax_task, states, prng_key,\n",
    "                                               inputs)\n",
    "\n",
    "\n",
    "def eval_step(states, prng_key, inputs):\n",
    "  states = states.to_eval_state()\n",
    "  return trainer_lib.eval_step_single_learner(jax_task, states, prng_key,\n",
    "                                              inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90"
   },
   "outputs": [],
   "source": [
    "key, train_key, eval_key = jax.random.split(key, 3)\n",
    "train_prng_seed = jax.random.split(train_key, num=jax.local_device_count())\n",
    "eval_prng_seed = jax.random.split(eval_key, num=jax.local_device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91"
   },
   "source": [
    "Note: When `pmap`ing a function, we can specify `donate_argnums` argument to save us HBM required to execute the function. The `donate_argnums` tells XLA that it can re-use the specified allocated input buffers to store the output of the function. This only works if the shape and type of both the input and output buffers exactly match. In our case, the both first argument to the `train_step` function and the first return type of `train_step_single_learner` are `train_states.TrainState` pytree types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92"
   },
   "outputs": [],
   "source": [
    "p_train_step = jax.pmap(train_step, donate_argnums=(0,), axis_name='batch')\n",
    "p_eval_step = jax.pmap(eval_step, axis_name='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93"
   },
   "source": [
    "Note: The following cell can take ~17 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "%timeit\n",
    "def aggregate_metrics(sum_metrics: WeightedScalars,\n",
    "                      batch_metrics: WeightedScalars) -> WeightedScalars:\n",
    "  \"\"\"Aggregate batch_metrics into sum_metrics.\"\"\"\n",
    "  # batch_metrics are aggregated and replicated, so we only need to take the\n",
    "  # values from the first device.\n",
    "  batch_metrics = jax.tree.map(lambda x: x[0], batch_metrics)\n",
    "  for key, metric in batch_metrics.items():\n",
    "    value, weight = metric\n",
    "    sum_value, sum_weight = sum_metrics.get(key, (0, 0))\n",
    "    sum_metrics[key] = sum_value + value * weight, sum_weight + weight\n",
    "  return sum_metrics\n",
    "\n",
    "\n",
    "def print_metrics(sum_metrics: WeightedScalars) -> None:\n",
    "  \"\"\"Helper function to print metrics to console.\"\"\"\n",
    "  for key, metric in sum_metrics.items():\n",
    "    print(f'  {key}: ({metric[0] / metric[1]}, {metric[1]})')\n",
    "\n",
    "\n",
    "# Now start the training loop.\n",
    "for i in range(40):\n",
    "  # One epoch over cached_train_batch.\n",
    "  sum_metrics = {}\n",
    "  # Rebatch training data to reduce overfitting\n",
    "  train_batches = make_batches(train_examples)\n",
    "  for batch, (input_ids, target_ids) in enumerate(train_batches):\n",
    "    if input_ids.shape[0] != BATCH_SIZE * num_devices:\n",
    "      continue\n",
    "    inputs = prepare_input_batch(input_ids, target_ids)\n",
    "    (replicated_jax_states, weighted_loss, mean_metrics, per_example_out,\n",
    "     summary_tensors) = p_train_step(replicated_jax_states, train_prng_seed,\n",
    "                                     inputs)\n",
    "    sum_metrics = aggregate_metrics(sum_metrics, mean_metrics)\n",
    "\n",
    "    # WeightedScalars are always a pair (value, weight pair)\n",
    "    per_batch_acc, per_batch_npreds = mean_metrics['mean_acc']\n",
    "    # In data-parallel training, metrics have already been synchronized across\n",
    "    # different model replicas, so we just need to fetch value from the first\n",
    "    # device.\n",
    "    per_batch_acc = per_batch_acc[0]\n",
    "    per_batch_npreds = per_batch_npreds[0]\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'epoch: {i} batch: {batch}, loss: {weighted_loss[0]},'\n",
    "            f' accuracy: {per_batch_acc}, num predictions: {per_batch_npreds}')\n",
    "\n",
    "  print(f'Train metrics for epoch: {i}')\n",
    "  print_metrics(sum_metrics)\n",
    "\n",
    "  # One epoch over cached_val_batch\n",
    "  sum_metrics = {}\n",
    "  val_batches = make_batches(val_examples)\n",
    "  for batch, (input_ids, target_ids) in enumerate(val_batches):\n",
    "    if input_ids.shape[0] != BATCH_SIZE * num_devices:\n",
    "      continue\n",
    "    inputs = prepare_input_batch(input_ids, target_ids)\n",
    "    (weighted_loss, mean_metrics, per_example_out,\n",
    "     summary_tensors) = p_eval_step(replicated_jax_states, eval_prng_seed,\n",
    "                                    inputs)\n",
    "    sum_metrics = aggregate_metrics(sum_metrics, mean_metrics)\n",
    "  # Compute per epoch stats\n",
    "  print(f'Eval metrics for epoch: {i}')\n",
    "  print_metrics(sum_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95"
   },
   "source": [
    "# Inference\n",
    "\n",
    "We have now trained our model. Let's test it out to see if we can translate a sentence.\n",
    "\n",
    "But before we do that, we need to get the variables first. Our Translator weights are replicated across 8 TPUs. The one liner below copies over one of these replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96"
   },
   "outputs": [],
   "source": [
    "final_vars = jax.device_get(\n",
    "    jax.tree.map(lambda x: x[0], replicated_jax_states.mdl_vars))\n",
    "\n",
    "# A flax module's bind method creates a stateful object with the params bound\n",
    "model = jax_task.model.bind(final_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97"
   },
   "outputs": [],
   "source": [
    "val_batches = make_batches(val_examples)\n",
    "pt_example, en_example = next(iter(val_batches))\n",
    "\n",
    "# Only take one single example\n",
    "pt_example = np.int64(pt_example[:1])\n",
    "en_example = np.int64(en_example[:1])\n",
    "\n",
    "pt_example_str = tokenizers.pt.detokenize(pt_example)\n",
    "en_example_str = tokenizers.en.detokenize(en_example)\n",
    "print(f'pt_example: {pt_example_str}')\n",
    "print(f'en_example: {en_example_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98"
   },
   "source": [
    "Below we implement a simple greedy decoding loop. It runs quite slowly.\n",
    "\n",
    "Homework: how would you make it run faster? What is the reason for the slowness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99"
   },
   "outputs": [],
   "source": [
    "def Inference(model: base_model.BaseModel,\n",
    "              tokenizers: Any,\n",
    "              sentence: str,\n",
    "              max_length: int = 20) -> tf.Tensor:\n",
    "  # Only bound models have a stateful scope bound through `bind` calls.\n",
    "  assert model.scope is not None, 'model should have params bound'\n",
    "\n",
    "  # This is the input sentence.\n",
    "  encoder_input = sentence\n",
    "\n",
    "  # Because English is the target lang, the first token to the transformer\n",
    "  # should be the english start token.\n",
    "  start_end = tokenizers.en.tokenize([''])[0].numpy()\n",
    "  # This is the start of sentence token.\n",
    "  start = start_end[0][np.newaxis]\n",
    "  # This is the end of sentence token.\n",
    "  end = start_end[1][np.newaxis]\n",
    "  output = np.array([start])\n",
    "\n",
    "  print(f'encoder_input: {encoder_input.shape}')\n",
    "  context_p = base_layer.JaxContext.HParams(do_eval=True)\n",
    "\n",
    "  with base_layer.JaxContext.new_context(hparams=context_p):\n",
    "    translator_bound = model.tt\n",
    "    encoder_paddings = (encoder_input == 0).astype(np.float32)\n",
    "    encoder_out = translator_bound.encoder_fprop(encoder_input,\n",
    "                                                 encoder_paddings)\n",
    "    print(f'encoder_out: {encoder_out.shape}')\n",
    "    for i in range(max_length):\n",
    "      print(f'target: {output.shape}')\n",
    "      target = output\n",
    "      target_paddings = np.zeros_like(target)\n",
    "      decoder_out = translator_bound.decoder_fprop(target, target_paddings,\n",
    "                                                   encoder_out,\n",
    "                                                   encoder_paddings)\n",
    "      metrics = translator_bound.decoder_softmax(decoder_out, target,\n",
    "                                                 target_paddings)\n",
    "      predicted_labels = metrics['per_example_argmax'][0]\n",
    "      print(f'predicted_labels: {predicted_labels}')\n",
    "      # select the last token from the seq_len dimension\n",
    "      predicted_id = predicted_labels[-1].astype(np.int32)\n",
    "      output = np.append(output, [[predicted_id]], axis=1)\n",
    "      print(f'output: {output}')\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "  text = tokenizers.en.detokenize(output)  # shape: ()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "100"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "%timeit\n",
    "en_translation = Inference(model, tokenizers, pt_example)\n",
    "print(en_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
